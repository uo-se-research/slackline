"""Grammar fuzzing with mutant search to find performance issues.

The approach is based on Nautilus:
    - We maintain derivation trees, not (just) strings generated by the grammar.
    - A "chunk store" keeps subtrees previously generated.  (We store all and only
    subtrees that have either achieved new coverage or a greater individual edge count
    after warming up with some random trees).
    - A tree may be mutated by "splicing" a previously generated subtree at any non-terminal
    symbol.

Some differences from Nautilus, because we are looking for performance issues:
    - A "good" sentence may be one that executes some control flow edge more than has been
    previously observed (even if the edge is not being executed for the first time or a new
    bucket of counts a la AFL).  Nautilus looks for new coverage and does not judge an edge
    execution count of 313 to be significant if we have already seen an edge execution count of 311.
    (In this we follow PerfFuzz.)
    - We limit the length of generated sentences.  Finding new edge counts because the input is longer
    is not interesting when looking for performance is
    - For the same reason, we do not attempt to minimize inputs, as most coverage-based fuzzers do.
    - We do not apply string mutations like Havoc.  "Almost correct" input may be very useful for finding
    security bugs, but for performance problems we want to generate (to the extent practical) correct inputs.

Other differences and possible differences from Nautilus:
    - The input grammars are more restrictive.  Nautilus supports Python scripts in place of CFG constructs.
    We support standard context-free grammars (though specified in an extended BNF).
    - When we cannot find a suitable splice, we result to generating a new random sentence.
    - We have not carefully studied Natilus's tactics for managing the "interesting" trees,
    and have probably not replicated it.  Our approach is based more on AFL, which simply iterates
    through all previously found "good" trees.
"""

import context

import mutation.mutator  as mutator
import mutation.gen_tree as gen_tree
import gramm.llparse
from gramm.char_classes import CharClasses
from gramm.unit_productions import UnitProductions
from mutation.dup_checker import History

from mutation.fake_runner import InputHandler  # STUB
# from targetAppConnect import InputHandler    # REAL

import logging
logging.basicConfig()
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)

import argparse

def cli() -> object:
    """Command line argument is path to grammar file"""

    parser = argparse.ArgumentParser("Mutating and splicing derivation trees")
    parser.add_argument("grammar", type=argparse.FileType("r"))
    parser.add_argument("--limit", type=int, default=60,
                        help="Upper bound on generated sentence length")
    parser.add_argument("--tokens", help="Limit by token count",
                        action="store_true")
    return parser.parse_args()

def ready_grammar(f) -> gramm.grammar.Grammar:
    gram = gramm.llparse.parse(f, len_based_size=True)
    gram.finalize()
    xform = UnitProductions(gram)
    xform.transform_all_rhs(gram)
    xform = CharClasses(gram)
    xform.transform_all_rhs(gram)
    return gram


def search(gram: gramm.grammar.Grammar, limit: int):
    input_handler = InputHandler()
    if not input_handler.is_connected():
        print(f"ERROR: No connection to input handler")
        exit(1)

    stale = History()
    frontier: list[mutation.gen_tree.DTreeNode] = []   # Trees on the frontier, to be mutated
    # Seed the frontier with ten random (but distinct) trees
    while len(frontier) < 10:
        t = gen_tree.derive(gram)
        txt = str(t)
        if stale.is_dup(txt):
            log.debug(f"Seeding, duplicated '{txt}'")
        else:
            log.debug(f"Fresh seed '{txt}'")
            frontier.append(t)
            mutator.stash(t)

    # Classic mutation search:  Repeat cycling through examples on the frontier,
    # generating new mutants from them.  Any mutant that is new and achieves some
    # progress is added to the frontier.
    #
    for _ in range(100):   # FIXME: Should this be "while (True)" for a time-limited run?
        found_good = False
        for basis in frontier:
            # Note Python 3 documentation seems to say we can append to
            # the list while iterating it.
            mutant = mutator.hybrid(basis, limit)
            if mutant is None:
                log.debug(f"Failed to hybridize '{basis}', try mutating instead")
                mutant = mutator.mutant(basis, limit)
            if mutant is None:
                log.debug(f"Failed to mutate '{basis}'")
            elif stale.is_dup(str(mutant)):
                log.debug(f"Mutant '{mutant}' is stale")
            else:
                log.debug(f"Generated valid mutant to test: '{mutant}'")
                stale.record(str(mutant))
                if test_mutant(str(mutant), input_handler):
                    found_good = True
                    frontier.append(mutant)
                    mutator.stash(mutant)
                    # Concurrent with iteration, so we may mutate the mutant
                    # before finishing this scan of the frontier!
            if not found_good:
                log.debug(f"Complete cycle without generating a good mutant")
    log.debug(mutator.SEEN)


max_cost = 0
max_hot = 0

def test_mutant(mut: str, input_handler: InputHandler) -> bool:
    """Tests a valid mutant,
    True if it is good (new coverage, new max, etc)
    side effect writes record of good mutant to output
    """
    global max_hot
    global max_cost
    tot_cost, new_bytes, new_max, hot_spot = input_handler.run_input(mut)
    if tot_cost > max_cost:
        max_cost = tot_cost
        log.debug(f"New total cost {tot_cost} for '{mut}")
        print(f"TOT {tot_cost}: {mut}")
    elif hot_spot > max_hot:
        log.debug(f"New hot spot {hot_spot} for '{mut}'")
        max_hot = hot_spot
        print(f"HOT {max_hot}: {mut}")
    elif new_max or new_bytes:
        log.debug(f"New coverage or edge max for '{mut}'")
        print(f"COV: {mut}")


def main():
    args = cli()
    limit = args.limit
    gram = ready_grammar(args.grammar)
    search(gram, limit)


if __name__ == "__main__":
    main()




