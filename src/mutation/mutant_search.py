"""Grammar fuzzing with mutant search to find performance issues.

The approach is based on Nautilus:
    - We maintain derivation trees, not (just) strings generated by the grammar.
    - A "chunk store" keeps subtrees previously generated.  (We store all and only
    subtrees that have either achieved new coverage or a greater individual edge count
    after warming up with some random trees).
    - A tree may be mutated by "splicing" a previously generated subtree at any non-terminal
    symbol.

Some differences from Nautilus, because we are looking for performance issues:
    - A "good" sentence may be one that executes some control flow edge more than has been
    previously observed (even if the edge is not being executed for the first time or a new
    bucket of counts a la AFL).  Nautilus looks for new coverage and does not judge an edge
    execution count of 313 to be significant if we have already seen an edge execution count of 311.
    (In this we follow PerfFuzz.)
    - We limit the length of generated sentences.  Finding new edge counts because the input is longer
    is not interesting when looking for performance is
    - For the same reason, we do not attempt to minimize inputs, as most coverage-based fuzzers do.
    - We do not apply string mutations like Havoc.  "Almost correct" input may be very useful for finding
    security bugs, but for performance problems we want to generate (to the extent practical) correct inputs.

Other differences and possible differences from Nautilus:
    - The input grammars are more restrictive.  Nautilus supports Python scripts in place of CFG constructs.
    We support standard context-free grammars (though specified in an extended BNF).
    - When we cannot find a suitable splice, we result to generating a new random subtree
    - We have not carefully studied Natilus's tactics for managing the "interesting" trees,
    and have probably not replicated it precisely.  Our approach is based more on AFL,
    which simply iterates through all previously found "good" trees.
"""

from typing import List
# For logging results:
import datetime
import time
import pathlib
import os
import math

from typing import Optional

import gramm.llparse
from gramm.char_classes import CharClasses
from gramm.unit_productions import UnitProductions
import mutation.search_config as search_config
import mutation.search as search

from targetAppConnect import InputHandler    # REAL



import logging
logging.basicConfig()
log = logging.getLogger(__name__)
log.setLevel(logging.WARN)

import slack

import logging
logging.basicConfig()
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)

import argparse

SEP = ":"   # For Linux.  In MacOS you may need another character (or maybe not)

def ready_grammar(f) -> gramm.grammar.Grammar:
    gram = gramm.llparse.parse(f, len_based_size=True)
    gram.finalize()
    xform = UnitProductions(gram)
    xform.transform_all_rhs(gram)
    xform = CharClasses(gram)
    xform.transform_all_rhs(gram)
    return gram


def cli() -> object:
    """Command line interface, including information for logging"""
    parser = argparse.ArgumentParser(description="Mutating and splicing derivation trees")
    parser.add_argument("app", type=str,
                        help="Application name, e.g., graphviz")
    parser.add_argument("grammar", type=str,
                        help="Path to grammar file")
    parser.add_argument("directory", type=str,
                        help="Root directory for experiment results")
    parser.add_argument("--length", type=int, default=60,
                        help="Upper bound on generated sentence length")
    parser.add_argument("--seconds", type=int, default=60 * 60,
                        help="Timeout in seconds, default 3600 (60 minutes)")
    parser.add_argument("--tokens", help="Limit by token count",
                        action="store_true")
    parser.add_argument("--runs", type=int, default=1,
                        help="How many times we should run the same experiment?")
    parser.add_argument("--slack", help="Report experiment to Slack",
                        action="store_true")
    parser.add_argument("--search", type=str,
                        help="bfs (breadth-first) or mcw (monte-carlo weighted)",
                        required=True,
                        choices = ["bfs", "mcw"])
    return parser.parse_args()


def create_result_directory(root: str, app: str, gram_name: str) -> pathlib.Path:
    """root should be a path to an existing writeable directory.
    Returns path to a writeable "list" subdirectory within a labeled subdirectory within root.
    May throw exception if directories cannot be created!
    """
    now = datetime.datetime.now()
    ident = f"app{SEP}{app}-gram{SEP}{gram_name}-crtime{SEP}{int(time.time())}"
    exp_path = pathlib.Path(root).joinpath(ident)
    os.mkdir(exp_path)
    list_path = exp_path.joinpath("list")
    list_dir = os.mkdir(list_path)
    log.info(f"Logging to {list_dir}")
    return list_path


def slack_message(m: str):
    slack.post_message_to_slack(f"{m}")


def slack_command(c: str):
    slack.post_message_to_slack(f"```\n{c}\n```")


def main():
    args = cli()
    length_limit: int = args.length
    gram_path = pathlib.Path(args.grammar)
    gram_name = gram_path.name
    gram = ready_grammar(open(args.grammar, "r"))
    timeout_ms = args.seconds * 1000
    number_of_exper = int(args.runs)
    report_to_slack = bool(args.slack)
    search_strategy = args.search
    log.info(f"Experiment {args.seconds} seconds with {args.search}")
    for run_id in range(1, number_of_exper+1):
        logdir = create_result_directory(args.directory, args.app, gram_name)
        if report_to_slack:
            slack_message(f"New mutant run #{run_id} out of {number_of_exper}.")
            slack_message(f"Configs: length=`{length_limit}`, gram_path=`{gram_path}`, gram_name=`{gram_name}`, "
                          f"duration(s)=`{args.seconds}`, logdir=`{logdir}`, tokens=`{args.tokens}`",
                          f"strategy {args.search}")
        search_config.init(strategy=search_strategy)
        searcher = search.Search(gram, logdir,
                                 InputHandler(search_config.FUZZ_SERVER, search_config.FUZZ_PORT),
                                 frontier=search_config.FRONTIER
                                 )
        searcher.search(length_limit, timeout_ms)
        searcher.summarize(length_limit, timeout_ms)
        if report_to_slack:
            slack_message(f"Run #{run_id} finished!")
            slack_command(search.report())


if __name__ == "__main__":
    main()




